# Real-Time-Sign-Language-Translation

A real-time system that detects and translates sign language gestures into spoken words using deep learning and computer vision. The project aims to improve communication accessibility for individuals with hearing or speech impairments.

---

## ðŸ§  Overview

This project uses a trained YOLOv5 model along with MediaPipe to detect sign language gestures from live webcam input. Detected gestures are mapped to corresponding words or phrases and converted into speech using a text-to-speech engine.

---

## ðŸŽ¯ Key Features

- Real-time sign detection using YOLOv5
- Hand and gesture tracking with MediaPipe
- Live webcam input processing using OpenCV
- Text-to-speech conversion for detected words
- Supports basic words like "yes", "no", "help", "ok", etc.

---

## ðŸ”§ Technologies Used

- **Python 3.x**
- **YOLOv5** â€“ for custom gesture detection
- **MediaPipe** â€“ for hand and pose tracking
- **OpenCV** â€“ for video streaming and frame capture
- **pyttsx3** / **gTTS** â€“ for speech synthesis
- **NumPy**, **Pandas** â€“ for data handling

---


